Convolutional(
  (convolutional): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (4): ReLU()
    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fullyconnected): Sequential(
    (0): Linear(in_features=7744, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): Sigmoid()
    (4): Linear(in_features=128, out_features=22, bias=True)
  )
)
 Epoch 1/20
-----------------------------------------------------------------
--- Training Loop --- 
Loss: 3.125018, Acc: 3.1% [    0/ 4750] 
Avg Loss: 1.994690, Avg Acc: 22.7% 

 --- Validation Loop --- 
 Loss: 2.134535, Acc: 46.9% [    0/ 1196] 
Avg Loss: 1.994365, Avg Acc: 51.3% 

 --- Confusion Matrix --- 
[[ 31   0   0   0  29   0   0   0  11   0   0   1   0   0   1   0   0   0   0   1   4]
 [  0   0   0   0  26   0   0   0   5   0   0   0   0   0   4   0   1   0   3   1   0]
 [ 10   0   6   0   2   0   3   0  22   0   0   0   2   0   0   0   0   0   0   0   0]
 [  3   0   0   0  26   0   0   0   2   0   0   0   0   0   0   0   4   0   3   0   2]
 [  0   0   0   0 139   0   0   0   6   0   0   0   0   0   0   0   1   0   1   0   0]
 [  0   0   0   0   0   0  32   0  15   0   0   0   0   0   3   0   0   0   2   2   0]
 [  0   0   0   0   1   0  63   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  34   0   0   0   0   0   0   2   0   0   0   0   1   0   0   1   1]
 [  0   0   0   0   1   0   1   0  63   0   0   0   0   0   1   0   0   0   2   0   0]
 [  0   0   0   0   1   0   3   0   0  19   0   0   0   0   0   0  15   0   1   0   0]
 [  0   0   0   0   0   0   0   0   0   0  73   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18   0   0   0   0   1   0   6   0   0   0   0  10   0   1   0  12]
 [  1   0   0   0   3   2  23   0   4   3   0   0   0   0   0   0   0   0   0   3   0]
 [  0   0   0   0  31   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2]
 [  0   0   0   0   3   0   2   0   3   0   0   0   0   0  48   0   6   0   0   0   0]
 [  0   0   0   0   6   0   5   0   0  12   0   0   4   0   0   0   5   0   3   1   3]
 [  0   0   0   0   2   0   0   0   0   0   0   0   0   0   3   0  58   0   0   0   1]
 [  0   0   0   0   7   0   0   0   0   0   0   0   0   0   1   0  37   0   0   1   1]
 [  0   0   0   0  13   0   3   0   3   1   0   0   1   0   6   0   0   0  35   0   0]
 [  0   0   0   0  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0  45   0]
 [  0   0   0   0  19   0   0   0   0   0   0   0   0   0   2   0   5   0   0   0  28]]

 --- Classification Report --- 
              precision    recall  f1-score   support

           0       0.69      0.40      0.50        78
           1       1.00      0.00      0.00        40
           2       1.00      0.13      0.24        45
           3       1.00      0.00      0.00        40
           4       0.37      0.95      0.53       147
           5       0.00      0.00      0.00        54
           6       0.47      0.98      0.63        64
           8       1.00      0.00      0.00        39
           9       0.47      0.93      0.62        68
          10       0.53      0.49      0.51        39
          11       1.00      1.00      1.00        73
          12       0.67      0.12      0.21        48
          13       0.00      0.00      0.00        39
          14       1.00      0.00      0.00        33
          15       0.70      0.77      0.73        62
          16       1.00      0.00      0.00        39
          17       0.41      0.91      0.56        64
          18       1.00      0.00      0.00        47
          19       0.69      0.56      0.62        62
          20       0.82      0.74      0.78        61
          21       0.52      0.52      0.52        54

    accuracy                           0.51      1196
   macro avg       0.68      0.40      0.35      1196
weighted avg       0.64      0.51      0.43      1196
